# Les 11 M√©thodes OSINT Compl√®tes üöÄ

*Date : 12 d√©cembre 2025*

## üìã Vue d'ensemble

Le pipeline d'enrichissement OSINT utilise maintenant **11 m√©thodes compl√®tes** inspir√©es du script local de l'utilisateur, pour collecter le maximum d'informations sur chaque entreprise.

---

## ‚úÖ M√©thodes impl√©ment√©es

### 1. **theHarvester** ‚úÖ
**Fonction :** Collecte d'emails via 4 sources fiables
- Sources : `bing`, `duckduckgo`, `yahoo`, `brave`
- Parsing JSON propre avec fallback texte
- Filtrage avanc√© (exclusion noreply, abuse@, etc.)
- **R√©sultat :** Emails du domaine

**Code :** `run_email_tools()`

---

### 2. **Web Scraping (About/Team/Contact)** ‚úÖ
**Fonction :** Scrape les pages web pour trouver emails et noms d'employ√©s
- Pages test√©es : `/`, `/about`, `/about-us`, `/team`, `/contact`, `/staff`, `/employees`, `/people`, `/equipe`, `/a-propos`
- Extraction emails via regex
- Extraction noms depuis balises `h1-h4`, `strong`, `b`, et attributs `data-name`
- **R√©sultat :** Emails + noms d'employ√©s

**Code :** `run_web_scraping()`

**D√©pendances :** `requests`, `beautifulsoup4`

---

### 3. **Extraction PDF** ‚úÖ
**Fonction :** T√©l√©charge et parse les PDFs trouv√©s sur le site
- D√©tection automatique des liens PDF sur la page principale
- Limite : 5 PDFs, 5 pages par PDF
- Extraction emails et noms depuis le texte des PDFs
- **R√©sultat :** Emails + noms d'employ√©s depuis PDFs

**Code :** `run_pdf_extraction()`

**D√©pendances :** `requests`, `beautifulsoup4`, `PyPDF2`

---

### 4. **Google Dorks** ‚úÖ
**Fonction :** Recherches Google cibl√©es pour trouver emails et employ√©s
- Utilise Selenium/Firefox en mode headless (si disponible)
- Fallback sur `requests` si Selenium non disponible
- Dorks utilis√©s :
  - `site:domain "@domain"`
  - `site:domain "email" OR "contact"`
  - `"company_name" "@domain"`
- **R√©sultat :** Emails + noms d'employ√©s depuis r√©sultats Google

**Code :** `run_google_dorks()`

**D√©pendances :** `selenium` (optionnel), `requests`, `beautifulsoup4`

---

### 5. **Subdomain Scraping manuel** ‚úÖ
**Fonction :** Teste les sous-domaines communs pour trouver des emails
- Sous-domaines test√©s : `www`, `mail`, `webmail`, `blog`, `news`, `newsletter`, `contact`, `about`, `team`, `careers`, `jobs`
- Scrape chaque sous-domaine pour extraire les emails
- **R√©sultat :** Emails trouv√©s sur les sous-domaines

**Code :** `run_subdomain_scraping()`

**D√©pendances :** `requests`, `beautifulsoup4`

---

### 6. **Subfinder** ‚úÖ
**Fonction :** D√©couverte automatique de sous-domaines
- Utilise l'outil `subfinder` en ligne de commande
- Sources : toutes les sources passives disponibles
- Timeout : 180s
- **R√©sultat :** Liste de sous-domaines (jusqu'√† 100)

**Code :** `run_subfinder()`

**D√©pendances :** `subfinder` (outil syst√®me)

---

### 7. **Wayback Machine** ‚úÖ
**Fonction :** Archives historiques du site
- Interroge Internet Archive via API CDX
- Limite : 50 URLs, filtr√©es √† 20 uniques
- Nettoyage : suppression trailing slashes, pr√©f√©rence HTTPS
- **R√©sultat :** URLs historiques archiv√©es

**Code :** `run_wayback()`

**D√©pendances :** `curl` (outil syst√®me)

---

### 8. **WHOIS Enhanced** ‚úÖ
**Fonction :** Extraction emails et noms depuis WHOIS
- Utilise l'outil `whois` en ligne de commande
- Extraction emails via regex
- Extraction noms depuis `Registrant Name`, `Admin Name`, `Tech Name`
- Filtrage : garde seulement les lignes importantes (registrar, dates, name servers, status, organization)
- **R√©sultat :** Emails + noms + donn√©es WHOIS brutes (30 lignes max)

**Code :** `run_whois_enhanced()`

**D√©pendances :** `whois` (outil syst√®me)

---

### 9. **R√©seaux sociaux** ‚úÖ
**Fonction :** Scrape les pages de r√©seaux sociaux mentionn√©es
- Support : LinkedIn, Facebook, Twitter/X
- Utilise les liens depuis la colonne `social_links` de la BDD
- Limite : 3 liens sociaux
- Extraction emails et noms depuis les m√©tadonn√©es
- **R√©sultat :** Emails + noms d'employ√©s depuis r√©seaux sociaux

**Code :** `run_social_media_scraping()`

**D√©pendances :** `requests`, `beautifulsoup4`

---

### 10. **Commentaires HTML** ‚úÖ
**Fonction :** Extraction emails depuis commentaires HTML et attributs
- Parse les commentaires HTML (`<!-- ... -->`)
- Parse les attributs `data-*` et `aria-*`
- Extraction emails via regex
- **R√©sultat :** Emails trouv√©s dans le code source

**Code :** `run_html_comments()`

**D√©pendances :** `requests`, `beautifulsoup4` (optionnel)

---

### 11. **GitHub Scraping** ‚úÖ
**Fonction :** Recherche dans les repositories GitHub
- Recherche par nom de domaine
- Parse les r√©sultats de recherche
- Parse les README des repositories (limite : 3 repos)
- Extraction emails depuis le texte
- **R√©sultat :** Emails trouv√©s sur GitHub

**Code :** `run_github_scraping()`

**D√©pendances :** `requests`, `beautifulsoup4`

---

### 12. **Robots.txt/Sitemap** ‚úÖ
**Fonction :** Parse robots.txt et sitemap pour trouver des pages cach√©es
- Parse `/robots.txt` pour trouver les sitemaps
- Limite : 2 sitemaps, 10 URLs par sitemap
- Scrape les pages contenant `about`, `team`, `contact`, `staff`
- Extraction emails depuis ces pages
- **R√©sultat :** Emails trouv√©s sur des pages cach√©es

**Code :** `run_robots_sitemap()`

**D√©pendances :** `requests`, `beautifulsoup4`

---

## üìä Colonnes BDD ajout√©es

| Colonne | Type | Description |
|---------|------|-------------|
| `osint_employees` | TEXT | Noms d'employ√©s trouv√©s (toutes sources) |
| `osint_html_comments` | TEXT | Emails depuis commentaires HTML |
| `osint_github_data` | TEXT | Donn√©es GitHub (emails) |
| `osint_social_data` | TEXT | Donn√©es r√©seaux sociaux (r√©sum√©) |

---

## üîÑ Fusion intelligente des donn√©es

### Emails
Tous les emails trouv√©s par les diff√©rentes m√©thodes sont **fusionn√©s automatiquement** dans `emails_osint` :
- theHarvester
- Web Scraping
- Extraction PDF
- Google Dorks
- Subdomain Scraping
- WHOIS Enhanced
- R√©seaux sociaux
- Commentaires HTML
- GitHub Scraping
- Robots.txt/Sitemap

**D√©duplication automatique** : les emails en double sont supprim√©s.

### Noms d'employ√©s
Tous les noms trouv√©s sont collect√©s dans `osint_employees` :
- Web Scraping
- Extraction PDF
- Google Dorks
- WHOIS Enhanced
- R√©seaux sociaux

---

## üì¶ D√©pendances Python

### Obligatoires
```bash
pip install requests beautifulsoup4 PyPDF2
```

### Optionnelles
```bash
pip install selenium  # Pour Google Dorks avec navigateur
```

### Outils syst√®me requis
- `curl` (pour Wayback Machine)
- `whatweb` (pour tech stack)
- `theHarvester` (pour emails)
- `subfinder` (pour sous-domaines)
- `whois` (pour infos domaine)

---

## üöÄ D√©ploiement sur le VPS

### 1. Installer les d√©pendances Python

```bash
cd ~/maps-scraper/osint-enricher
source venv/bin/activate  # Si vous utilisez un venv
pip install -r requirements.txt
```

### 2. Installer Selenium (optionnel, pour Google Dorks)

```bash
# Installer Firefox et geckodriver
sudo apt-get update
sudo apt-get install -y firefox-esr

# T√©l√©charger geckodriver
wget https://github.com/mozilla/geckodriver/releases/download/v0.33.0/geckodriver-v0.33.0-linux64.tar.gz
tar -xzf geckodriver-v0.33.0-linux64.tar.gz
sudo mv geckodriver /usr/local/bin/
sudo chmod +x /usr/local/bin/geckodriver
```

### 3. Mettre √† jour le code

```bash
cd ~/maps-scraper
git pull
```

### 4. Red√©marrer le service

```bash
sudo systemctl restart osint-enricher
```

### 5. V√©rifier les logs

```bash
sudo journalctl -u osint-enricher -f
```

---

## üìà Performance

### Temps d'ex√©cution estim√© par entreprise

| M√©thode | Temps moyen |
|---------|-------------|
| WhatWeb | 2-5s |
| theHarvester | 30-60s (4 sources √ó 90s max) |
| Web Scraping | 10-20s (10 pages) |
| Extraction PDF | 15-30s (5 PDFs) |
| Google Dorks | 20-40s (3 dorks) |
| Subdomain Scraping | 10-15s (11 subdomains) |
| Subfinder | 15-30s |
| WHOIS | 2-5s |
| Wayback | 3-8s |
| R√©seaux sociaux | 10-20s (3 liens) |
| Commentaires HTML | 2-5s |
| GitHub Scraping | 10-20s |
| Robots.txt/Sitemap | 10-20s |

**Total estim√© :** 2-5 minutes par entreprise (selon les donn√©es disponibles)

---

## üéØ R√©sultat attendu

### Exemple de logs

```
[2025-12-11 14:21:21] üîÑ Enrichissement #1/50
[2025-12-11 14:21:21] üìå ID: 12345 | Entreprise: Example Corp
[2025-12-11 14:21:21] üåê Site: https://example.com
[2025-12-11 14:21:23]   ‚úÖ WhatWeb: 5 tech(s)
[2025-12-11 14:21:53]   ‚úÖ theHarvester: 3 email(s) valide(s) pour example.com
[2025-12-11 14:22:13]   ‚úÖ Web Scraping: 2 email(s), 5 employ√©(s)
[2025-12-11 14:22:28]   ‚úÖ Extraction PDF: 1 email(s), 2 employ√©(s)
[2025-12-11 14:22:48]   ‚úÖ Google Dorks: 1 email(s), 0 employ√©(s)
[2025-12-11 14:22:58]   ‚úÖ Subdomain Scraping: 1 email(s)
[2025-12-11 14:23:13]   ‚úÖ Subfinder: 15 sous-domaine(s) unique(s)
[2025-12-11 14:23:15]   ‚úÖ WHOIS Enhanced: 2 email(s), 1 nom(s)
[2025-12-11 14:23:18]   ‚úÖ Wayback: 10 URLs uniques
[2025-12-11 14:23:28]   ‚úÖ R√©seaux sociaux: 0 email(s), 2 employ√©(s)
[2025-12-11 14:23:30]   ‚úÖ Commentaires HTML: 1 email(s)
[2025-12-11 14:23:40]   ‚úÖ GitHub Scraping: 0 email(s)
[2025-12-11 14:23:50]   ‚úÖ Robots.txt/Sitemap: 0 email(s)
[2025-12-11 14:23:50] üíæ Sauvegarde en BDD...
[2025-12-11 14:23:50]    ‚úÖ SAUVEGARDE R√âUSSIE pour ID 12345
[2025-12-11 14:23:50] ‚úÖ ID 12345 - Example Corp termin√© et sauvegard√© en BDD
```

### Donn√©es sauvegard√©es

- **emails_osint** : `contact@example.com, info@example.com, admin@example.com, ...` (fusion de toutes les sources)
- **osint_employees** : `John Doe, Jane Smith, ...` (tous les noms trouv√©s)
- **osint_html_comments** : `hidden@example.com` (emails depuis commentaires)
- **osint_github_data** : `dev@example.com` (emails GitHub)
- **osint_social_data** : `Emails: 0, Employ√©s: 2` (r√©sum√© r√©seaux sociaux)

---

## üîß Gestion des erreurs

### D√©pendances manquantes

Si une d√©pendance est manquante, la m√©thode correspondante est **silencieusement ignor√©e** :
- `REQUESTS_AVAILABLE = False` ‚Üí Web Scraping, PDF, etc. d√©sactiv√©s
- `BS4_AVAILABLE = False` ‚Üí Parsing HTML d√©sactiv√©
- `PDF_AVAILABLE = False` ‚Üí Extraction PDF d√©sactiv√©e
- `SELENIUM_AVAILABLE = False` ‚Üí Google Dorks utilise `requests` (moins efficace)

### Timeouts

Chaque m√©thode a un timeout appropri√© :
- Requ√™tes web : 10-15s
- theHarvester : 90s par source
- Subfinder : 180s global
- WHOIS : 30s

### Rate limiting

Des d√©lais sont ajout√©s entre les requ√™tes pour √©viter le rate limiting :
- Entre pages web : 1s
- Entre sources theHarvester : 1s
- Entre Google Dorks : 5s
- Entre r√©seaux sociaux : 2s

---

## üìù Notes techniques

### Fusion des emails

Les emails sont collect√©s dans des `set()` Python pour √©viter les doublons, puis fusionn√©s dans une seule cha√Æne s√©par√©e par des virgules.

### Extraction de noms

Les noms sont d√©tect√©s via regex : `^[A-Z][a-z]+(?:\s+[A-Z][a-z]+){1,2}$`
- 2-3 mots
- Premi√®re lettre en majuscule
- Longueur : 3-50 caract√®res

### Filtrage des emails

Exclusion automatique des emails g√©n√©riques :
- `noreply`, `no-reply`, `donotreply`
- `abuse@`, `postmaster@`, `hostmaster@`, `webmaster@`
- Domaines de test : `example.com`, `test.com`, etc.

---

## üéâ R√©sum√©

**11 m√©thodes OSINT compl√®tes** sont maintenant int√©gr√©es dans le pipeline d'enrichissement, permettant de collecter :
- ‚úÖ **Emails** depuis 10 sources diff√©rentes
- ‚úÖ **Noms d'employ√©s** depuis 5 sources diff√©rentes
- ‚úÖ **Sous-domaines** via 2 m√©thodes
- ‚úÖ **Technologies web** via WhatWeb
- ‚úÖ **Archives historiques** via Wayback Machine
- ‚úÖ **Infos domaine** via WHOIS

**R√©sultat :** Enrichissement OSINT **complet et exhaustif** pour chaque entreprise ! üöÄ

