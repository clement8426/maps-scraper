# ğŸ—ºï¸ Maps Scraper - Scraper Google Maps pour entreprises tech en Suisse Romande

## ğŸ“‹ Table des matiÃ¨res

- [Vue d'ensemble](#vue-densemble)
- [Architecture](#architecture)
- [Stack technique](#stack-technique)
- [Configuration VPS](#configuration-vps)
- [Installation](#installation)
- [Utilisation](#utilisation)
- [Structure du projet](#structure-du-projet)
- [Base de donnÃ©es](#base-de-donnÃ©es)
- [Services et processus](#services-et-processus)
- [SÃ©curitÃ©](#sÃ©curitÃ©)
- [Maintenance](#maintenance)
- [DÃ©pannage](#dÃ©pannage)

---

## ğŸ¯ Vue d'ensemble

Application web complÃ¨te de scraping Google Maps pour rÃ©cupÃ©rer automatiquement les informations d'entreprises tech en Suisse Romande (canton de NeuchÃ¢tel en prioritÃ©).

### FonctionnalitÃ©s

- âœ… Scraping automatique Google Maps (nom, adresse, tÃ©lÃ©phone, site web, note, avis)
- âœ… Validation DNS des emails (rejet des emails fictifs)
- âœ… Extraction emails depuis les sites web des entreprises
- âœ… Rotation des User-Agents pour Ã©viter la dÃ©tection
- âœ… SystÃ¨me de checkpoint (reprise automatique aprÃ¨s interruption)
- âœ… Interface web avec dashboard temps rÃ©el
- âœ… Filtres avancÃ©s (ville, prÃ©sence site/email)
- âœ… Export CSV avec filtres appliquÃ©s
- âœ… Logs en temps rÃ©el du scraping
- âœ… RÃ©cupÃ©ration automatique en cas d'erreur navigateur
- âœ… Base de donnÃ©es SQLite
- âœ… Authentication HTTP Basic
- âœ… Responsive (mobile, tablette, desktop)

---

## ğŸ—ï¸ Architecture

### Architecture globale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Internet                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VPS Ubuntu 25.04                          â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Nginx (port 80)                                     â”‚  â”‚
â”‚  â”‚  - Reverse proxy                                     â”‚  â”‚
â”‚  â”‚  - HTTP Basic Auth (.htpasswd)                      â”‚  â”‚
â”‚  â”‚  - Gestion SSL (si configurÃ©)                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                     â”‚                                        â”‚
â”‚                     â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Gunicorn (127.0.0.1:5000)                          â”‚  â”‚
â”‚  â”‚  - 2 workers                                         â”‚  â”‚
â”‚  â”‚  - WSGI Server                                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                     â”‚                                        â”‚
â”‚                     â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Flask Application (backend/app.py)                  â”‚  â”‚
â”‚  â”‚  - API REST                                          â”‚  â”‚
â”‚  â”‚  - Gestion scraper                                   â”‚  â”‚
â”‚  â”‚  - Streaming logs                                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                            â”‚                      â”‚
â”‚         â–¼                            â–¼                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  Frontend   â”‚            â”‚  Backend         â”‚          â”‚
â”‚  â”‚  (HTML/CSS) â”‚            â”‚  scraper_suisse  â”‚          â”‚
â”‚  â”‚  /JS)       â”‚            â”‚  _romande.py     â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                      â”‚                      â”‚
â”‚                                      â–¼                      â”‚
â”‚                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚                             â”‚  companies.db    â”‚           â”‚
â”‚                             â”‚  (SQLite)        â”‚           â”‚
â”‚                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flux de donnÃ©es

```
1. Utilisateur â†’ Nginx (auth) â†’ Gunicorn â†’ Flask â†’ Interface web
2. Clic "DÃ©marrer" â†’ Flask lance scraper_suisse_romande.py
3. Scraper â†’ Google Maps â†’ Extraction donnÃ©es â†’ SQLite
4. Interface web â†’ API Flask â†’ Lecture SQLite â†’ Affichage temps rÃ©el
```

---

## ğŸ› ï¸ Stack technique

### Backend
- **Python 3.13** (compatible)
- **Flask 3.0.0** - Framework web
- **Gunicorn 21.2.0** - WSGI server
- **Playwright 1.48.0** - Automatisation navigateur (Firefox + Chromium)
- **BeautifulSoup4** - Parsing HTML
- **Pandas 2.2.0+** - Manipulation donnÃ©es
- **SQLite3** - Base de donnÃ©es
- **dnspython** - Validation DNS emails
- **email-validator** - Validation emails

### Frontend
- **HTML5/CSS3/JavaScript** (Vanilla, pas de framework)
- **Design responsive** (mobile-first)
- **Fetch API** pour les appels AJAX

### Infrastructure
- **Nginx** - Reverse proxy + authentification
- **Systemd** - Gestion des services
- **UFW** - Firewall
- **Git** - Versioning

### Serveur
- **OS** : Ubuntu 25.04 (Plucky)
- **Utilisateur** : `ubuntu`
- **RÃ©pertoire** : `/home/ubuntu/maps-scraper`

---

## âš™ï¸ Configuration VPS

### Ã‰tat actuel du VPS

#### Services actifs

```bash
# Service principal (web interface)
scraper-web.service
â”œâ”€ Status: active (running)
â”œâ”€ Port: 127.0.0.1:5000
â”œâ”€ User: ubuntu
â”œâ”€ WorkingDirectory: /home/ubuntu/maps-scraper/backend
â””â”€ Command: gunicorn --bind 127.0.0.1:5000 app:app --workers 2
```

#### Ports ouverts (UFW)

| Port | Service | Description |
|------|---------|-------------|
| 22   | SSH     | AccÃ¨s administrateur |
| 80   | HTTP    | Interface web (Nginx) |
| 443  | HTTPS   | SSL (si configurÃ©) |

#### Structure fichiers VPS

```
/home/ubuntu/maps-scraper/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py                      # API Flask
â”‚   â”œâ”€â”€ scraper_suisse_romande.py  # Script scraping
â”‚   â”œâ”€â”€ companies.db                # Base de donnÃ©es SQLite
â”‚   â”œâ”€â”€ checkpoint.json             # Progression scraping
â”‚   â”œâ”€â”€ intermediate_data.csv       # DonnÃ©es temporaires
â”‚   â”œâ”€â”€ scraper.log                 # Logs du scraper
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ clean_and_deduce_emails.py
â”‚       â””â”€â”€ verify_emails.py
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html                  # Interface web
â”‚   â”œâ”€â”€ style.css                   # Styles
â”‚   â””â”€â”€ script.js                   # Logique frontend
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ install.sh                  # Script installation VPS
â”‚   â””â”€â”€ change_password.py          # Utilitaire changement mot de passe
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ INSTALL.md                  # Documentation installation
â”‚   â”œâ”€â”€ UPDATE.md                   # ProcÃ©dure mise Ã  jour
â”‚   â””â”€â”€ CHANGE_PASSWORD.md          # Changer mot de passe admin
â”œâ”€â”€ venv/                           # Environnement virtuel Python
â”œâ”€â”€ requirements.txt                # DÃ©pendances Python
â”œâ”€â”€ .env                            # Variables d'environnement
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

#### Configuration Nginx

**Fichier** : `/etc/nginx/sites-available/scraper`

```nginx
server {
    listen 80;
    server_name _;

    # Authentification HTTP Basic
    auth_basic "Scraper Admin";
    auth_basic_user_file /etc/nginx/.htpasswd;

    location / {
        proxy_pass http://127.0.0.1:5000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # Pour le streaming des logs
        proxy_buffering off;
        proxy_read_timeout 300s;
    }
}
```

#### Configuration Systemd

**Fichier** : `/etc/systemd/system/scraper-web.service`

```ini
[Unit]
Description=Scraper Google Maps - Web Interface
After=network.target

[Service]
Type=notify
User=ubuntu
Group=ubuntu
WorkingDirectory=/home/ubuntu/maps-scraper/backend
Environment="PATH=/home/ubuntu/maps-scraper/venv/bin"
EnvironmentFile=/home/ubuntu/maps-scraper/.env
ExecStart=/home/ubuntu/maps-scraper/venv/bin/gunicorn --bind 127.0.0.1:5000 app:app --workers 2
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

#### Variables d'environnement (.env)

```bash
WEB_USERNAME=admin
WEB_PASSWORD=VotreMotDePasseIci
DATABASE_PATH=companies.db
FLASK_ENV=production
```

---

## ğŸ“¦ Installation

### Installation initiale sur VPS

```bash
# 1. Se connecter au VPS
ssh ubuntu@<IP_VPS>

# 2. Cloner le projet
git clone https://github.com/votre-compte/maps-scraper.git
cd maps-scraper

# 3. Lancer l'installation automatique
sudo ./scripts/install.sh
```

Le script `install.sh` effectue :
- âœ… Mise Ã  jour du systÃ¨me
- âœ… Installation dÃ©pendances systÃ¨me (libxml2, libxslt, pkg-config, etc.)
- âœ… Installation Python 3.13 + pip
- âœ… CrÃ©ation environnement virtuel
- âœ… Installation dÃ©pendances Python
- âœ… Installation Playwright (Firefox + Chromium)
- âœ… Configuration Nginx (reverse proxy + auth)
- âœ… CrÃ©ation service systemd
- âœ… Configuration firewall (UFW)
- âœ… GÃ©nÃ©ration `.env` avec identifiants

### Installation en local (dÃ©veloppement)

```bash
# 1. Cloner le projet
git clone https://github.com/votre-compte/maps-scraper.git
cd maps-scraper

# 2. CrÃ©er l'environnement virtuel
python3 -m venv venv
source venv/bin/activate

# 3. Installer les dÃ©pendances
pip install -r requirements.txt

# 4. Installer Playwright
playwright install firefox chromium

# 5. CrÃ©er le fichier .env
cp env.example .env
# Modifier .env avec vos identifiants

# 6. Lancer l'application
./start_local.sh
```

AccÃ¨s : `http://localhost:8080`

---

## ğŸš€ Utilisation

### AccÃ¨s Ã  l'interface web

**URL** : `http://<IP_VPS>`

**Identifiants** : Ceux configurÃ©s dans `.env` lors de l'installation

### FonctionnalitÃ©s de l'interface

#### 1. Dashboard
- Statistiques temps rÃ©el
  - Total entreprises
  - Avec site web
  - Avec email
  - DerniÃ¨re mise Ã  jour
- Top 10 villes
- ContrÃ´le du scraper (DÃ©marrer/ArrÃªter)
- Statut et progression

#### 2. Filtres
- Par ville (dropdown)
- Avec site web (checkbox)
- Avec email (checkbox)
- Application en temps rÃ©el

#### 3. Liste des entreprises
- Tableau avec toutes les donnÃ©es
- Liens cliquables (site web, Maps)
- Tri par colonnes
- Export CSV avec filtres

#### 4. Logs en temps rÃ©el
- Affichage des logs du scraper
- Scroll automatique
- Couleurs selon type (success, error, warning, info)
- Bouton "Voir les logs"

### Commandes serveur

```bash
# Voir le statut du service
sudo systemctl status scraper-web

# RedÃ©marrer le service
sudo systemctl restart scraper-web

# ArrÃªter le service
sudo systemctl stop scraper-web

# DÃ©marrer le service
sudo systemctl start scraper-web

# Voir les logs en temps rÃ©el
sudo journalctl -u scraper-web -f

# Voir les logs du scraper
tail -f ~/maps-scraper/backend/scraper.log

# VÃ©rifier si le scraper est actif
ps aux | grep scraper_suisse_romande.py
```

---

## ğŸ“Š Base de donnÃ©es

### Structure SQLite (companies.db)

```sql
CREATE TABLE companies (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    company_name TEXT NOT NULL,
    maps_link TEXT UNIQUE,
    city TEXT,
    tag TEXT,                    -- Mot-clÃ© de recherche
    address TEXT,
    phone TEXT,
    website TEXT,
    rating REAL,                 -- Note Google (0-5)
    reviews_count INTEGER,       -- Nombre d'avis
    email TEXT,                  -- Emails validÃ©s (sÃ©parÃ©s par virgule)
    social_links TEXT,           -- Liens sociaux (LinkedIn, etc.)
    status TEXT,                 -- 'Harvested', 'Enriched', etc.
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### AccÃ¨s direct Ã  la BDD

```bash
cd ~/maps-scraper/backend
sqlite3 companies.db

# Commandes SQLite utiles
.tables                          # Lister les tables
.schema companies                # Voir la structure
SELECT COUNT(*) FROM companies;  # Nombre total
SELECT * FROM companies LIMIT 5; # Premiers rÃ©sultats
.exit                            # Quitter
```

---

## ğŸ”§ Services et processus

### Service web (scraper-web.service)

**RÃ´le** : Lance et maintient l'interface web Flask

**Gestion** :
```bash
sudo systemctl start scraper-web    # DÃ©marrer
sudo systemctl stop scraper-web     # ArrÃªter
sudo systemctl restart scraper-web  # RedÃ©marrer
sudo systemctl status scraper-web   # Statut
sudo systemctl enable scraper-web   # Auto-dÃ©marrage au boot
```

**Logs** :
```bash
sudo journalctl -u scraper-web -f   # Logs temps rÃ©el
sudo journalctl -u scraper-web -n 100  # 100 derniÃ¨res lignes
```

### Processus de scraping

**DÃ©marrage** : Via l'interface web (bouton "DÃ©marrer")

**Script** : `backend/scraper_suisse_romande.py`

**Logs** : `backend/scraper.log`

**CaractÃ©ristiques** :
- Tourne en arriÃ¨re-plan (subprocess)
- IndÃ©pendant du service web
- Continue mÃªme si vous fermez votre navigateur
- Peut Ãªtre arrÃªtÃ© via l'interface web

**VÃ©rification manuelle** :
```bash
# Voir si le scraper tourne
ps aux | grep scraper_suisse_romande.py

# ArrÃªter manuellement (si nÃ©cessaire)
pkill -f scraper_suisse_romande.py
```

---

## ğŸ” SÃ©curitÃ©

### Authentification

**Type** : HTTP Basic Authentication (Nginx)

**Fichier** : `/etc/nginx/.htpasswd`

**Format** : `username:password_hash`

### Changer le mot de passe

**MÃ©thode 1 : Script automatique**
```bash
cd ~/maps-scraper
python3 scripts/change_password.py
sudo systemctl restart scraper-web
```

**MÃ©thode 2 : Manuel**
```bash
cd ~/maps-scraper
nano .env
# Modifier WEB_PASSWORD=NouveauMotDePasse
sudo htpasswd -c /etc/nginx/.htpasswd admin
sudo systemctl restart nginx
sudo systemctl restart scraper-web
```

### Firewall (UFW)

```bash
# Voir les rÃ¨gles actives
sudo ufw status

# Ouvrir un port (si besoin)
sudo ufw allow 81/tcp

# Fermer un port
sudo ufw delete allow 81/tcp
```

### Bonnes pratiques

- âœ… Ne jamais committer le fichier `.env`
- âœ… Utiliser des mots de passe forts (12+ caractÃ¨res)
- âœ… Changer les identifiants par dÃ©faut
- âœ… Maintenir le systÃ¨me Ã  jour (`apt update && apt upgrade`)
- âœ… Surveiller les logs rÃ©guliÃ¨rement
- âš ï¸ Ne pas exposer la BDD SQLite publiquement

---

## ğŸ”„ Maintenance

### Mise Ã  jour du code

**Sur votre machine locale** :
```bash
cd ~/test/maps-scrap
git add .
git commit -m "Description des changements"
git push origin main
```

**Sur le VPS** :
```bash
cd ~/maps-scraper
git pull origin main

# Si nouvelles dÃ©pendances
source venv/bin/activate
pip install -r requirements.txt

# Recharger systemd si service modifiÃ©
sudo systemctl daemon-reload

# RedÃ©marrer le service
sudo systemctl restart scraper-web
```

ğŸ“– **Guide complet** : `docs/UPDATE.md`

### Sauvegarde de la base de donnÃ©es

```bash
# CrÃ©er une sauvegarde
cp ~/maps-scraper/backend/companies.db ~/companies_backup_$(date +%Y%m%d).db

# TÃ©lÃ©charger la BDD en local (depuis votre machine)
scp ubuntu@<IP_VPS>:~/maps-scraper/backend/companies.db ./companies_local.db
```

### Nettoyage

```bash
# Supprimer les fichiers temporaires
cd ~/maps-scraper/backend
rm -f checkpoint.json intermediate_data.csv scraper.log

# Vider la BDD (âš ï¸ ATTENTION)
sqlite3 companies.db "DELETE FROM companies;"
```

---

## ğŸ› DÃ©pannage

### ProblÃ¨mes courants

#### 1. Service ne dÃ©marre pas

```bash
# Voir les erreurs
sudo journalctl -u scraper-web -n 50

# VÃ©rifier le port 5000
sudo lsof -i :5000

# Tester manuellement
cd ~/maps-scraper/backend
source ../venv/bin/activate
python app.py
```

#### 2. Erreur "Not Found" sur l'interface

**Cause** : Nginx ou Gunicorn mal configurÃ©

**Solution** :
```bash
# VÃ©rifier Nginx
sudo nginx -t
sudo systemctl restart nginx

# VÃ©rifier le service
sudo systemctl status scraper-web
```

#### 3. Scraper ne dÃ©marre pas

**VÃ©rifier** :
```bash
# Logs du scraper
tail -f ~/maps-scraper/backend/scraper.log

# Playwright installÃ© ?
cd ~/maps-scraper
source venv/bin/activate
playwright install firefox chromium
```

#### 4. Erreur "Executable doesn't exist"

**Cause** : Navigateurs Playwright non installÃ©s

**Solution** :
```bash
cd ~/maps-scraper
source venv/bin/activate
playwright install firefox chromium
playwright install-deps firefox chromium  # NÃ©cessite sudo
```

#### 5. Erreur d'authentification

**Solution** : RÃ©gÃ©nÃ©rer le `.htpasswd`
```bash
sudo htpasswd -c /etc/nginx/.htpasswd admin
# Entrer le mÃªme mot de passe que dans .env
sudo systemctl restart nginx
```

#### 6. Base de donnÃ©es corrompue

```bash
# VÃ©rifier l'intÃ©gritÃ©
sqlite3 ~/maps-scraper/backend/companies.db "PRAGMA integrity_check;"

# RecrÃ©er la BDD (âš ï¸ perte de donnÃ©es)
rm ~/maps-scraper/backend/companies.db
# Relancer le scraper pour recrÃ©er
```

---

## ğŸ“š Documentation

- **Installation** : `docs/INSTALL.md`
- **Mise Ã  jour** : `docs/UPDATE.md`
- **Changement mot de passe** : `docs/CHANGE_PASSWORD.md`
- **Structure projet** : `PROJECT_STRUCTURE.md`

---

## ğŸ¯ Zones de scraping

### Canton de NeuchÃ¢tel (prioritÃ©)
- NeuchÃ¢tel, La Chaux-de-Fonds, Le Locle
- Val-de-Ruz, Val-de-Travers, Fleurier
- Cernier, Peseux, Colombier
- Marin-Epagnier, Saint-Blaise, Boudry, Cressier

### Villes proches (hors canton)
- Yverdon-les-Bains, Pontarlier, Morteau, BesanÃ§on

### Autres Suisse Romande
- GenÃ¨ve, Lausanne, Fribourg, Sion
- Nyon, Renens, Meyrin, Plan-les-Ouates
- Martigny, Vevey, Montreux
- DelÃ©mont, Porrentruy

---

## ğŸ” Mots-clÃ©s de recherche

### DÃ©veloppement web & digital
- Agence Web, DÃ©veloppement logiciel, Conception de sites web
- CrÃ©ation site internet, Agence digitale, Web design
- DÃ©veloppeur web, IntÃ©grateur web, UX Designer

### DÃ©veloppement spÃ©cialisÃ©
- Full Stack, Frontend developer, Backend developer
- App development, Mobile app, Application mobile
- E-commerce, Site e-commerce, Boutique en ligne

### Software & SaaS
- Ã‰diteur de logiciels, Software development, SaaS company
- Startup tech, Tech startup, Scale-up

### SÃ©curitÃ© & infrastructure
- CybersÃ©curitÃ©, SÃ©curitÃ© informatique, Consultant IT
- Consultant informatique, Services informatiques entreprises
- Cloud provider, DevOps, Infrastructure IT

### Marketing digital
- SEO, RÃ©fÃ©rencement web, Marketing digital
- Social media management, Community manager

### Data & IA
- Data science, Intelligence artificielle, Machine Learning
- Big Data, Data analyst

---

## ğŸ“ Support

Pour toute question ou problÃ¨me :
1. Consulter les docs (`docs/`)
2. VÃ©rifier les logs (`sudo journalctl -u scraper-web -f`)
3. Regarder dans le dÃ©pannage ci-dessus

---

## ğŸ“ Licence

Projet privÃ© - Tous droits rÃ©servÃ©s

---

## ğŸ”® Ã‰volutions futures possibles

- [ ] Second bot d'enrichissement (LinkedIn, Pappers, etc.)
- [ ] Export Excel en plus du CSV
- [ ] Filtres avancÃ©s (par note Google, nombre d'avis)
- [ ] Notifications email en fin de scraping
- [ ] API REST publique
- [ ] Dashboard analytics avancÃ©
- [ ] Gestion multi-utilisateurs
- [ ] SystÃ¨me de tags personnalisÃ©s
- [ ] DÃ©tection automatique des doublons
- [ ] IntÃ©gration CRM (HubSpot, Salesforce)

---

**Version** : 1.0.0  
**DerniÃ¨re mise Ã  jour** : DÃ©cembre 2024  
**Auteur** : Votre nom/sociÃ©tÃ©
