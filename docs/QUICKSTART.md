# ‚ö° Quick Start - D√©ploiement VPS en 5 minutes

## üéØ Pour d√©ployer sur votre VPS

### 1. Connexion SSH √† votre VPS
```bash
ssh root@VOTRE_IP_VPS
```

### 2. Installation automatique
```bash
# Installer git
apt-get update && apt-get install -y git

# Cloner le projet
git clone https://github.com/VOTRE_USERNAME/maps-scrap.git
cd maps-scrap

# Lancer l'installation (tout est automatique)
chmod +x scripts/install.sh
sudo ./scripts/install.sh
```

### 3. Connexion √† l'interface

L'installation vous donnera l'URL et les identifiants :
```
URL: http://XXX.XXX.XXX.XXX
Username: admin
Password: VOTRE_MOT_DE_PASSE
```

Ouvrez votre navigateur et c'est parti ! üöÄ

---

## üìã Ce que fait le script d'installation

‚úÖ Installe automatiquement :
- Python 3 + pip
- Node.js
- Nginx (serveur web)
- Playwright + Firefox
- Toutes les d√©pendances Python
- Service systemd (d√©marrage automatique)
- Firewall s√©curis√© (ufw)
- Base de donn√©es SQLite

‚úÖ Configure automatiquement :
- Reverse proxy Nginx
- Authentification double (Nginx + Flask)
- Permissions utilisateur
- Service qui d√©marre au boot
- Ports firewall (22, 80, 443)

‚úÖ Cr√©e automatiquement :
- Utilisateur syst√®me `scraper`
- Base de donn√©es `companies.db`
- Fichier de configuration `.env`
- Mot de passe s√©curis√© (ou g√©n√©r√© automatiquement)

**Dur√©e totale : 5-10 minutes**

---

## üéÆ Utilisation rapide

### Depuis l'interface web

1. **Ouvrir** http://VOTRE_IP dans votre navigateur
2. **S'authentifier** avec admin/VOTRE_MOT_DE_PASSE
3. **Cliquer** sur "‚ñ∂Ô∏è D√©marrer" pour lancer le scraper
4. **Attendre** que les donn√©es se remplissent (visible en temps r√©el)
5. **Filtrer** par ville, email, site web
6. **Exporter** en CSV avec le bouton "üì• Exporter CSV"

### Commandes utiles

```bash
# Voir le statut du service
sudo systemctl status scraper-web

# Voir les logs en temps r√©el
sudo journalctl -u scraper-web -f

# Red√©marrer le service
sudo systemctl restart scraper-web

# Arr√™ter le service
sudo systemctl stop scraper-web

# Acc√©der √† la base de donn√©es
cd /home/scraper/maps-scraper/backend
sqlite3 companies.db
```

---

## üîß Personnalisation rapide

### Changer le mot de passe

```bash
cd /home/scraper/maps-scraper
nano .env

# Modifier :
WEB_PASSWORD=nouveau_mot_de_passe

# Red√©marrer
sudo systemctl restart scraper-web
```

### Ajouter des villes

```bash
nano /home/scraper/maps-scraper/backend/scraper_suisse_romande.py

# Ligne ~20, ajouter vos villes dans CITIES = [...]
# Sauvegarder et quitter (Ctrl+X, Y, Enter)
```

### Exporter toutes les donn√©es

```bash
cd /home/scraper/maps-scraper/backend
sqlite3 companies.db ".mode csv" ".output export.csv" "SELECT * FROM companies;"
# Le fichier export.csv est cr√©√©
```

---

## ‚ö†Ô∏è Important

### S√©curit√©
- **Changez le mot de passe** apr√®s l'installation
- Le firewall est activ√© (seuls SSH et HTTP sont ouverts)
- Double authentification activ√©e (Nginx + Flask)

### Performance
- Le scraper peut tourner pendant **8-12 heures** (toutes les combinaisons)
- Vous pouvez l'arr√™ter/reprendre √† tout moment
- Les donn√©es sont sauvegard√©es en temps r√©el

### L√©gal
- Usage personnel/√©ducatif uniquement
- Respectez les CGU de Google Maps
- Respectez le RGPD et la LPD suisse

---

## üÜò En cas de probl√®me

### Le service ne d√©marre pas
```bash
sudo journalctl -u scraper-web -n 50
sudo systemctl restart scraper-web
```

### Impossible d'acc√©der √† l'interface
```bash
sudo systemctl status nginx
sudo systemctl restart nginx
sudo ufw status
```

### R√©initialiser compl√®tement
```bash
cd /home/scraper/maps-scraper/backend
rm companies.db checkpoint.json intermediate_data.csv
sudo systemctl restart scraper-web
```

---

## üìñ Documentation compl√®te

- **README.md** : Guide utilisateur complet
- **DEPLOY.md** : Guide de d√©ploiement d√©taill√©
- **README_COMPLET.md** : Documentation technique

---

**Besoin d'aide ? Consultez DEPLOY.md pour plus de d√©tails !**

