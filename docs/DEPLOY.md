# ğŸš€ Guide de DÃ©ploiement VPS

Guide complet pour dÃ©ployer le scraper sur un VPS avec interface web accessible Ã  distance.

## ğŸ“‹ PrÃ©requis

- **VPS** avec Ubuntu 20.04+ ou Debian 11+
- **2 GB RAM minimum** (4 GB recommandÃ©)
- **20 GB espace disque**
- AccÃ¨s **root** ou **sudo**

## ğŸ¯ Installation en 3 Ã©tapes

### 1ï¸âƒ£ Cloner le repository sur le VPS

**Tu peux cloner le projet n'importe oÃ¹ !** Le script d'installation va automatiquement copier les fichiers vers `/home/scraper/maps-scraper`.

```bash
# Connexion SSH au VPS (en tant que ubuntu ou root)
ssh ubuntu@VOTRE_IP_VPS

# Installation de git si nÃ©cessaire
sudo apt-get update && sudo apt-get install -y git

# Cloner le projet (n'importe oÃ¹, par exemple dans /home/ubuntu)
cd ~
git clone https://github.com/VOTRE_USERNAME/maps-scrap.git
cd maps-scrap

# OU cloner dans /tmp, /opt, etc. - peu importe !
# Le script install.sh va copier vers /home/scraper/maps-scraper automatiquement
```

### 2ï¸âƒ£ Lancer l'installation automatique

```bash
# Rendre le script exÃ©cutable
chmod +x scripts/install.sh

# Lancer l'installation (en tant que root)
sudo ./scripts/install.sh
```

Le script va :
- âœ… Installer Python 3, Node.js, Nginx
- âœ… CrÃ©er un utilisateur systÃ¨me `scraper`
- âœ… Installer toutes les dÃ©pendances
- âœ… Installer Playwright + Firefox
- âœ… Configurer Nginx (reverse proxy)
- âœ… CrÃ©er un service systemd
- âœ… Configurer le firewall
- âœ… Vous demander de crÃ©er un mot de passe

**DurÃ©e** : 5-10 minutes

### 3ï¸âƒ£ AccÃ©der Ã  l'interface web

Ã€ la fin de l'installation, vous obtiendrez :

```
============================================
âœ… Installation terminÃ©e !
============================================

ğŸ“ Informations de connexion:

   URL: http://XXX.XXX.XXX.XXX
   Nom d'utilisateur: admin
   Mot de passe: VOTRE_MOT_DE_PASSE

============================================
```

Ouvrez votre navigateur et accÃ©dez Ã  l'URL indiquÃ©e !

## ğŸ”’ SÃ©curitÃ©

### Double authentification
Le systÃ¨me utilise **2 couches de sÃ©curitÃ©** :

1. **Authentification Nginx** (HTTP Basic Auth)
2. **Authentification Flask** (vÃ©rification dans l'application)

### Firewall configurÃ©
- Port 22 (SSH) : Ouvert
- Port 80 (HTTP) : Ouvert
- Port 443 (HTTPS) : Ouvert
- Tout le reste : FermÃ©

### Recommandations
```bash
# Changer le port SSH (optionnel mais recommandÃ©)
nano /etc/ssh/sshd_config
# Modifier: Port 2222
systemctl restart sshd

# Mettre Ã  jour le firewall
ufw allow 2222/tcp
ufw delete allow 22/tcp
```

## ğŸ› ï¸ Gestion du service

### Commandes principales

```bash
# DÃ©marrer le service
sudo systemctl start scraper-web

# ArrÃªter le service
sudo systemctl stop scraper-web

# RedÃ©marrer le service
sudo systemctl restart scraper-web

# Voir le statut
sudo systemctl status scraper-web

# Voir les logs en temps rÃ©el
sudo journalctl -u scraper-web -f
```

### DÃ©marrage automatique

Le service dÃ©marre automatiquement au boot du VPS.

Pour dÃ©sactiver :
```bash
sudo systemctl disable scraper-web
```

## ğŸ“Š Utilisation de l'interface web

### Tableau de bord
- **Statistiques** : Total entreprises, avec site web, avec email
- **Top 10 villes** : Graphique des villes les plus reprÃ©sentÃ©es
- **ContrÃ´le scraper** : DÃ©marrer/ArrÃªter le scraping
- **Filtres** : Ville, site web, email, recherche
- **Export CSV** : TÃ©lÃ©charger les donnÃ©es

### Lancer un scraping

1. Cliquez sur **"â–¶ï¸ DÃ©marrer"** dans la section "ContrÃ´le du Scraper"
2. Le scraper s'exÃ©cute en arriÃ¨re-plan
3. La progression est visible sur le dashboard
4. Les donnÃ©es sont mises Ã  jour en temps rÃ©el

### Exporter les donnÃ©es

1. Appliquer les filtres souhaitÃ©s (ville, etc.)
2. Cliquer sur **"ğŸ“¥ Exporter CSV"**
3. Le fichier est tÃ©lÃ©chargÃ© automatiquement

## ğŸ—„ï¸ AccÃ¨s direct Ã  la base de donnÃ©es

```bash
# Se connecter en tant que scraper
sudo su - scraper
cd /home/scraper/maps-scraper/backend

# Ouvrir la base SQLite
sqlite3 companies.db

# Exemples de requÃªtes SQL
SELECT COUNT(*) FROM companies;
SELECT * FROM companies WHERE city = 'NeuchÃ¢tel' LIMIT 10;
SELECT city, COUNT(*) FROM companies GROUP BY city;

# Quitter
.quit
```

## ğŸ”„ Mise Ã  jour du code

```bash
cd /home/scraper/maps-scraper
git pull
sudo systemctl restart scraper-web
```

## ğŸ“ Structure des fichiers

```
/home/scraper/maps-scraper/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py                    # API Flask
â”‚   â”œâ”€â”€ scraper_suisse_romande.py # Scraper
â”‚   â”œâ”€â”€ companies.db              # Base de donnÃ©es SQLite
â”‚   â”œâ”€â”€ checkpoint.json           # Progression
â”‚   â””â”€â”€ intermediate_data.csv     # DonnÃ©es temporaires
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html                # Interface web
â”‚   â”œâ”€â”€ style.css                 # Styles
â”‚   â””â”€â”€ script.js                 # JavaScript
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ install.sh                # Installation
â”‚   â””â”€â”€ start.sh                  # DÃ©marrage manuel
â”œâ”€â”€ venv/                         # Environnement Python
â””â”€â”€ .env                          # Variables d'environnement
```

## ğŸ› DÃ©pannage

### Le service ne dÃ©marre pas

```bash
# Voir les erreurs
sudo journalctl -u scraper-web -n 50

# VÃ©rifier les permissions
sudo chown -R scraper:scraper /home/scraper/maps-scraper

# RedÃ©marrer
sudo systemctl restart scraper-web
```

### Impossible d'accÃ©der Ã  l'interface web

```bash
# VÃ©rifier que Nginx fonctionne
sudo systemctl status nginx

# VÃ©rifier la configuration
sudo nginx -t

# RedÃ©marrer Nginx
sudo systemctl restart nginx

# VÃ©rifier le firewall
sudo ufw status
```

### Le scraper ne trouve rien

```bash
# VÃ©rifier que Playwright et Firefox sont installÃ©s
cd /home/scraper/maps-scraper
source venv/bin/activate
playwright install firefox
playwright install-deps firefox
```

### RÃ©initialiser complÃ¨tement

```bash
# Supprimer les donnÃ©es
cd /home/scraper/maps-scraper/backend
rm companies.db checkpoint.json intermediate_data.csv

# RedÃ©marrer le service
sudo systemctl restart scraper-web
```

## ğŸŒ Configurer un nom de domaine (optionnel)

### 1. Pointer le domaine vers votre VPS

Dans votre registrar DNS, crÃ©ez un enregistrement A :
```
scraper.votredomaine.com  â†’  XXX.XXX.XXX.XXX
```

### 2. Modifier la configuration Nginx

```bash
sudo nano /etc/nginx/sites-available/scraper
```

Modifier la ligne `server_name` :
```nginx
server_name scraper.votredomaine.com;
```

### 3. Installer un certificat SSL (gratuit avec Let's Encrypt)

```bash
# Installer certbot
sudo apt-get install -y certbot python3-certbot-nginx

# Obtenir un certificat
sudo certbot --nginx -d scraper.votredomaine.com

# Renouvellement automatique
sudo systemctl enable certbot.timer
```

Votre site sera accessible en HTTPS ! ğŸ”’

## ğŸ’¡ Conseils de performance

### Pour un gros volume de donnÃ©es

1. **Augmenter les workers Gunicorn** :
```bash
sudo nano /etc/systemd/system/scraper-web.service
# Modifier: --workers 4
sudo systemctl daemon-reload
sudo systemctl restart scraper-web
```

2. **Optimiser SQLite** :
```sql
-- CrÃ©er des index pour les requÃªtes frÃ©quentes
CREATE INDEX idx_city ON companies(city);
CREATE INDEX idx_email ON companies(email);
CREATE INDEX idx_website ON companies(website);
```

3. **Configurer un cache Nginx** :
```nginx
proxy_cache_path /var/cache/nginx levels=1:2 keys_zone=my_cache:10m;
proxy_cache my_cache;
```

## ğŸ“ Support

En cas de problÃ¨me :
1. VÃ©rifiez les logs : `sudo journalctl -u scraper-web -f`
2. Consultez la documentation : `README.md`
3. Testez localement : `./scripts/start.sh`

---

**Bon scraping ! ğŸ•·ï¸**

